import os
from fastapi import FastAPI
import uvicorn
import requests
from datetime import datetime, timedelta
import pandas as pd
from tqdm import tqdm
from io import StringIO


basetime = ['0200', '0500', '0800', '1100', '1400', '1700', '2000', '2300']
url = 'http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst'
service_key = 'cnFWOksdH2rQuZ9YQs2IR3frMjm2kgy8eauRY4ujdTSTvGEeDGXulTzCIJtU7htSZeFnoof4l6RGh3EpVIbo1Q=='  # ì¸ì¦í‚¤ (URL Encode í•„ìš” ì—†ìŒ)
base_time = '0200'
nx = 37.5606111111111  # ì˜ˆë³´ì§€ì  X ì¢Œí‘œ
ny = 127.039  # ì˜ˆë³´ì§€ì  Y ì¢Œí‘œ

def calculate_base_time():
    """í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ base_time ë°˜í™˜"""
    now = datetime.now()
    current_hour = now.hour

    base_times = ['0200', '0500', '0800', '1100', '1400', '1700', '2000', '2300']
    base_hours = [2, 5, 8, 11, 14, 17, 20, 23]

    # í˜„ì¬ ì‹œê°ë³´ë‹¤ ì´ì „ì¸ ë°œí‘œ ì‹œê°ë“¤ ì¤‘ ê°€ì¥ ìµœê·¼ ê²ƒ
    for i in range(len(base_hours) - 1, -1, -1):  # ë’¤ì—ì„œë¶€í„° ê²€ìƒ‰
        if current_hour >= base_hours[i]:
            base_date = datetime.today().strftime('%Y%m%d')
            return base_date, base_times[i]


    if current_hour < 2:
        # í˜„ì¬ ì‹œê°ì´ 02:00ë³´ë‹¤ ì´ë¥´ë©´ ì „ë‚  23:00
        base_date = (now - timedelta(days=1)).strftime('%Y%m%d')
        return base_date, '2300'

def get_ultra_short_data(nx, ny, base_date, base_time):
    # ìš”ì²­ íŒŒë¼ë¯¸í„° êµ¬ì„±
    df_final = pd.DataFrame()  # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
    for i in range(1,2):
        params = {
            'serviceKey': service_key,
            'numOfRows': '1000',
            'pageNo': i,
            'dataType': 'JSON',  # JSON ë˜ëŠ” XML
            'base_date': base_date,
            'base_time': base_time,
            'nx': nx,
            'ny': ny
        }

        # ìš”ì²­ ë° ì‘ë‹µ ì²˜ë¦¬
        try:
            response = requests.get(url, params=params)
            if response.status_code == 200:
                data = response.json()  # JSON ì‘ë‹µ íŒŒì‹±
                result_json = data['response']['body']['items']['item']
                result_df = pd.DataFrame(result_json)
                df_final = pd.concat([df_final, result_df], ignore_index=True)  # ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°
            else:
                return ["ìš”ì²­ ì‹¤íŒ¨:", response.status_code]
        except Exception as e:
            print("âŒ ìš”ì²­ ì‹¤íŒ¨:", e)

    return df_final.to_json(force_ascii=False)  # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    df_final.to_csv('ultra_short_data.csv')  # CSV íŒŒì¼ë¡œ ì €ì¥

def get_short_term_data():
    base_date, base_time = calculate_base_time()
    params = {
        'serviceKey': service_key,  # ì¸ì¦í‚¤ (URL ì¸ì½”ë”© ì•ˆí•´ë„ ë¨)
        'numOfRows': '50',  # í•œ í˜ì´ì§€ ê²°ê³¼ ìˆ˜
        'pageNo': '1',  # í˜ì´ì§€ ë²ˆí˜¸
        'dataType': 'JSON',  # ì‘ë‹µ í˜•ì‹ (JSON or XML)
        'base_date': base_date,  # ë°œí‘œì¼ì (YYYYMMDD)
        'base_time': base_time,  # ë°œí‘œì‹œê° (HHMM)
        'nx': nx,  # ì˜ˆë³´ì§€ì  X ì¢Œí‘œ
        'ny': ny  # ì˜ˆë³´ì§€ì  Y ì¢Œí‘œ
    }

    response = requests.get(url, params=params)

    if response.status_code == 200:
        data = response.json()

    else:
        print("âŒ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸"""
    jobs = []
    for job in scheduler.get_jobs():
        jobs.append({
            "id": job.id,
            "name": job.name,
            "next_run": str(job.next_run_time),
            "trigger": str(job.trigger)
        })
    return {
        "running": scheduler.running,
        "jobs": jobs
    }


def download_ultra_short_data():
    print("ğŸ»ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    os.makedirs("data", exist_ok=True)  # ë°ì´í„° ì €ì¥ í´ë” ìƒì„±
    region_code_df = pd.read_csv('ì§€ì—­_ì½”ë“œ_ì •ë¦¬.csv', encoding='utf-8-sig')
    df_all_region = pd.DataFrame()  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•  ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”

    base_date, base_time = calculate_base_time()
    now_year = str(datetime.now().year)
    now_month = str(datetime.now().month)

    os.makedirs(os.path.join('data', now_year), exist_ok=True)  # ë°ì´í„° ì €ì¥ í´ë” ìƒì„±

    if os.path.exists(os.path.join('data', now_year, f"{now_year}_{now_month}.csv")):
        check_region_df = pd.read_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"),
                                      encoding='utf-8-sig')

        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        for index, row in tqdm(region_code_df.iterrows(),
                               total=len(region_code_df),
                               desc="ğŸŒ¤ï¸  ê¸°ìƒ ë°ì´í„° í™•ì¸ ì¤‘"):
            # ê° ì§€ì—­ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ
            nx, ny = row['ê²©ì X'], row['ê²©ì Y']
            try:
                region_df_check = check_region_df[(check_region_df['nx'] == nx) &
                                                  (check_region_df['ny'] == ny) &
                                                  (check_region_df['baseTime'] == base_time) &
                                                  (check_region_df['baseDate'] == base_date)]
            except:
                region_df_check = pd.DataFrame()

            try:
                if len(region_df_check) != 835 or len(region_df_check) != 943:
                    json_data = get_ultra_short_data(nx, ny, base_date, base_time)
                    data = pd.read_json(StringIO(json_data), orient='records')
                    data = data[data['category'] == 'SKY'].reset_index().drop(columns=['index'])  # 'SKY' ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
                    data['baseTime'] = data['baseTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
                    data['fcstTime'] = data['fcstTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°

                    df_all_region = pd.concat([df_all_region, data], ignore_index=True)  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„° í•©ì¹˜ê¸°
            except Exception as e:
                print(e)

    else:
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ (ì „ì²´ ë°ì´í„° ìˆ˜ì§‘)
        for index, row in tqdm(region_code_df.iterrows(),
                               total=len(region_code_df),
                               desc="ğŸŒ¤ï¸  ì „ì²´ ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì¤‘"):
            # ê° ì§€ì—­ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ
            nx, ny = row['ê²©ì X'], row['ê²©ì Y']
            data = pd.read_json(get_ultra_short_data(nx, ny, base_date, base_time), orient='records')
            data = data[data['category'] == 'SKY'].reset_index().drop(columns=['index'])  # 'SKY' ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
            data['baseTime'] = data['baseTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
            data['fcstTime'] = data['fcstTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°

            df_all_region = pd.concat([df_all_region, data], ignore_index=True)  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„° í•©ì¹˜ê¸°

    # ë°ì´í„° ì €ì¥ ê³¼ì •ì—ë„ ì§„í–‰ë¥  í‘œì‹œ (ì„ íƒì‚¬í•­)
    print("ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
    if not os.path.exists(os.path.join('data', now_year, f"{now_year}_{now_month}.csv")):
        df_all_region.to_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"), header=True, index=False)
    else:
        data_final = pd.read_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"))
        data_final = pd.concat([data_final, df_all_region], ignore_index=True)
        data_final.drop_duplicates(inplace=True)
        data_final.to_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"), index=False)

    print(f"ğŸ»âœ…ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - {base_date} {base_time} ê¸°ì¤€")

def main():
    download_ultra_short_data()

if __name__ == "__main__":
    main()