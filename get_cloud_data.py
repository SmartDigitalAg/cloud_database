import os
from fastapi import FastAPI
import uvicorn
import requests
from datetime import datetime, timedelta
import pandas as pd
from tqdm import tqdm
from io import StringIO
from zoneinfo import ZoneInfo  # Python 3.9 ì´ìƒ


basetime = ['0200', '0500', '0800', '1100', '1400', '1700', '2000', '2300']
url = 'http://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst'
service_key = 'cnFWOksdH2rQuZ9YQs2IR3frMjm2kgy8eauRY4ujdTSTvGEeDGXulTzCIJtU7htSZeFnoof4l6RGh3EpVIbo1Q=='  # ì¸ì¦í‚¤ (URL Encode í•„ìš” ì—†ìŒ)
base_time = '0200'
nx = 37.5606111111111  # ì˜ˆë³´ì§€ì  X ì¢Œí‘œ
ny = 127.039  # ì˜ˆë³´ì§€ì  Y ì¢Œí‘œ


def calculate_base_time():
    """í˜„ìž¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê°€ìž¥ ìµœê·¼ base_time ë°˜í™˜"""
    # now = datetime.now()
    now = datetime.now(ZoneInfo("Asia/Seoul"))

    current_hour = now.hour

    base_times = ['0200', '0500', '0800', '1100', '1400', '1700', '2000', '2300']
    base_hours = [2, 5, 8, 11, 14, 17, 20, 23]

    # í˜„ìž¬ ì‹œê°ë³´ë‹¤ ì´ì „ì¸ ë°œí‘œ ì‹œê°ë“¤ ì¤‘ ê°€ìž¥ ìµœê·¼ ê²ƒ
    for i in range(len(base_hours) - 1, -1, -1):  # ë’¤ì—ì„œë¶€í„° ê²€ìƒ‰
        if current_hour >= base_hours[i]:
            base_date = datetime.now(ZoneInfo("Asia/Seoul")).strftime('%Y%m%d')
            return base_date, base_times[i]

    if current_hour < 2:
        # í˜„ìž¬ ì‹œê°ì´ 02:00ë³´ë‹¤ ì´ë¥´ë©´ ì „ë‚  23:00
        base_date = (now - timedelta(days=1)).strftime('%Y%m%d')
        return base_date, '2300'


def get_ultra_short_data(nx, ny, base_date, base_time):
    # ìš”ì²­ íŒŒë¼ë¯¸í„° êµ¬ì„±
    df_final = pd.DataFrame()  # ìµœì¢… ë°ì´í„°í”„ë ˆìž„ ì´ˆê¸°í™”
    for i in range(1, 2):
        params = {
            'serviceKey': service_key,
            'numOfRows': '1000',
            'pageNo': i,
            'dataType': 'JSON',  # JSON ë˜ëŠ” XML
            'base_date': base_date,
            'base_time': base_time,
            'nx': nx,
            'ny': ny
        }
        response = requests.get(url, params=params)
        # ìš”ì²­ ë° ì‘ë‹µ ì²˜ë¦¬
        try:
            if response.status_code == 200:
                data = response.json()  # JSON ì‘ë‹µ íŒŒì‹±
                result_json = data['response']['body']['items']['item']
                # print(f"âœ… ìš”ì²­ ì„±ê³µ: {response.status_code} - íŽ˜ì´ì§€ {i}")
                result_df = pd.DataFrame(result_json)
                df_final = pd.concat([df_final, result_df], ignore_index=True)  # ë°ì´í„°í”„ë ˆìž„ í•©ì¹˜ê¸°
            else:
                return ["ìš”ì²­ ì‹¤íŒ¨:", response.status_code]
        except requests.exceptions.JSONDecodeError as e:
            print("âŒ JSON ë””ì½”ë”© ì‹¤íŒ¨:", e)
            print("ìš”ì²­ íŒŒë¼ë¯¸í„°", params)
            print("ì‘ë‹µ ë‚´ìš©:", response.text[:500])  # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            return None
        except requests.exceptions.RequestException as e:
            print("âŒ ìš”ì²­ ì‹¤íŒ¨:", e)
            return None
    return df_final.to_json(force_ascii=False)  # ìµœì¢… ë°ì´í„°í”„ë ˆìž„ ë°˜í™˜

def get_short_term_data():
    base_date, base_time = calculate_base_time()
    params = {
        'serviceKey': service_key,  # ì¸ì¦í‚¤ (URL ì¸ì½”ë”© ì•ˆí•´ë„ ë¨)
        'numOfRows': '50',  # í•œ íŽ˜ì´ì§€ ê²°ê³¼ ìˆ˜
        'pageNo': '1',  # íŽ˜ì´ì§€ ë²ˆí˜¸
        'dataType': 'JSON',  # ì‘ë‹µ í˜•ì‹ (JSON or XML)
        'base_date': base_date,  # ë°œí‘œì¼ìž (YYYYMMDD)
        'base_time': base_time,  # ë°œí‘œì‹œê° (HHMM)
        'nx': nx,  # ì˜ˆë³´ì§€ì  X ì¢Œí‘œ
        'ny': ny  # ì˜ˆë³´ì§€ì  Y ì¢Œí‘œ
    }

    response = requests.get(url, params=params)

    if response.status_code == 200:
        data = response.json()

    else:
        print("âŒ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)


def download_ultra_short_data():
    print("ðŸ»ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì‹œìž‘")
    os.makedirs("data", exist_ok=True)  # ë°ì´í„° ì €ìž¥ í´ë” ìƒì„±
    region_code_df = pd.read_csv('ì§€ì—­_ì½”ë“œ_ì •ë¦¬.csv', encoding='utf-8-sig')

    base_date, base_time = calculate_base_time()
    now_year = str(datetime.now(ZoneInfo("Asia/Seoul")).year)
    now_month = str(datetime.now(ZoneInfo("Asia/Seoul")).month)

    os.makedirs(os.path.join('data', now_year), exist_ok=True)  # ë°ì´í„° ì €ìž¥ í´ë” ìƒì„±

    if os.path.exists(os.path.join('data', now_year, f"{now_year}_{now_month}.csv")):
        already_save_df = pd.read_csv(os.path.join('data', now_year, f"{now_year}_{now_month}_ultra.csv"),
                                      encoding='utf-8-sig')

        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        for index, row in tqdm(region_code_df.iterrows(),
                               total=len(region_code_df),
                               desc="ðŸŒ¤ï¸  ê¸°ìƒ ë°ì´í„° í™•ì¸ ì¤‘"):
            # ê° ì§€ì—­ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ
            nx, ny = row['ê²©ìž X'], row['ê²©ìž Y']
            try:
                now_target_df = already_save_df[(already_save_df['nx'] == nx) &
                                                (already_save_df['ny'] == ny) &
                                                (already_save_df['baseTime'] == base_time) &
                                                (already_save_df['baseDate'] == base_date)]
            except:
                now_target_df = pd.DataFrame()

            try:
                if len(now_target_df) != 835 and len(now_target_df) != 943:
                    json_data = get_ultra_short_data(nx, ny, base_date, base_time)
                    data = pd.read_json(StringIO(json_data), orient='records')
            except:
                with open('error_log.txt', 'a') as f:
                    f.write(f"Error for nx: {nx}, ny: {ny} at {base_date} {base_time}\n")

            try:
                data = data[data['category'] == 'SKY'].reset_index().drop(columns=['index'])  # 'SKY' ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
                data['baseTime'] = data['baseTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
                data['fcstTime'] = data['fcstTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°

                already_save_df = pd.concat([already_save_df, data], ignore_index=True)  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„° í•©ì¹˜ê¸°
            except Exception as e:
                print(data)
                print(e)

        print("ðŸ’¾ ë°ì´í„° ì €ìž¥ ì¤‘...")
        already_save_df.to_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"), header=True, index=False)

    else:
        already_save_df = pd.DataFrame()  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„°ë¥¼ ì €ìž¥í•  ë°ì´í„°í”„ë ˆìž„ ì´ˆê¸°í™”
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ (ì „ì²´ ë°ì´í„° ìˆ˜ì§‘)
        for index, row in tqdm(region_code_df.iterrows(),
                               total=len(region_code_df),
                               desc="ðŸŒ¤ï¸  ì „ì²´ ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì¤‘"):
            # ê° ì§€ì—­ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ
            nx, ny = row['ê²©ìž X'], row['ê²©ìž Y']
            data = pd.read_json(get_ultra_short_data(nx, ny, base_date, base_time), orient='records')
            data = data[data['category'] == 'SKY'].reset_index().drop(columns=['index'])  # 'SKY' ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
            data['baseTime'] = data['baseTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
            data['fcstTime'] = data['fcstTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°

            already_save_df = pd.concat([already_save_df, data], ignore_index=True)  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„° í•©ì¹˜ê¸°

        print("ðŸ’¾ ë°ì´í„° ì €ìž¥ ì¤‘...")
        already_save_df.to_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"), header=True, index=False)

    print(f"ðŸ»âœ…ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - {base_date} {base_time} ê¸°ì¤€")


def download_short_term_data():
    print("ðŸ»ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì‹œìž‘")
    os.makedirs("data", exist_ok=True)  # ë°ì´í„° ì €ìž¥ í´ë” ìƒì„±
    region_code_df = pd.read_csv('ì§€ì—­_ì½”ë“œ_ì •ë¦¬.csv', encoding='utf-8-sig')

    df_all_region = pd.DataFrame()  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„°ë¥¼ ì €ìž¥í•  ë°ì´í„°í”„ë ˆìž„ ì´ˆê¸°í™”

    base_date, base_time = calculate_base_time()
    now_year = str(datetime.now(ZoneInfo("Asia/Seoul")).year)
    now_month = str(datetime.now(ZoneInfo("Asia/Seoul")).month)

    os.makedirs(os.path.join('data', now_year), exist_ok=True)  # ë°ì´í„° ì €ìž¥ í´ë” ìƒì„±

    if os.path.exists(os.path.join('data', now_year, f"{now_year}_{now_month}.csv")):
        check_region_df = pd.read_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"),
                                      encoding='utf-8-sig')

        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
        for index, row in tqdm(region_code_df.iterrows(),
                               total=len(region_code_df),
                               desc="ðŸŒ¤ï¸  ê¸°ìƒ ë°ì´í„° í™•ì¸ ì¤‘"):
            # ê° ì§€ì—­ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ
            nx, ny = row['ê²©ìž X'], row['ê²©ìž Y']
            try:
                region_df_check = check_region_df[(check_region_df['nx'] == nx) &
                                                  (check_region_df['ny'] == ny) &
                                                  (check_region_df['baseTime'] == base_time) &
                                                  (check_region_df['baseDate'] == base_date)]
            except:
                region_df_check = pd.DataFrame()

            try:
                if len(region_df_check) != 835 or len(region_df_check) != 943:
                    json_data = get_ultra_short_data(nx, ny, base_date, base_time)
                    data = pd.read_json(StringIO(json_data), orient='records')
                    data = data[data['category'] == 'SKY'].reset_index().drop(columns=['index'])  # 'SKY' ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
                    data['baseTime'] = data['baseTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
                    data['fcstTime'] = data['fcstTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°

                    df_all_region = pd.concat([df_all_region, data], ignore_index=True)  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„° í•©ì¹˜ê¸°
            except Exception as e:
                raise e

    else:
        # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ (ì „ì²´ ë°ì´í„° ìˆ˜ì§‘)
        for index, row in tqdm(region_code_df.iterrows(),
                               total=len(region_code_df),
                               desc="ðŸŒ¤ï¸  ì „ì²´ ê¸°ìƒ ë°ì´í„° ìˆ˜ì§‘ ì¤‘"):
            # ê° ì§€ì—­ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ
            nx, ny = row['ê²©ìž X'], row['ê²©ìž Y']
            data = pd.read_json(get_ultra_short_data(nx, ny, base_date, base_time), orient='records')
            data = data[data['category'] == 'SKY'].reset_index().drop(columns=['index'])  # 'SKY' ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë§Œ í•„í„°ë§
            data['baseTime'] = data['baseTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
            data['fcstTime'] = data['fcstTime'].astype(str).apply(lambda x: x.zfill(4))  # ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°

            df_all_region = pd.concat([df_all_region, data], ignore_index=True)  # ëª¨ë“  ì§€ì—­ì˜ ë°ì´í„° í•©ì¹˜ê¸°

    # ë°ì´í„° ì €ìž¥ ê³¼ì •ì—ë„ ì§„í–‰ë¥  í‘œì‹œ (ì„ íƒì‚¬í•­)
    print("ðŸ’¾ ë°ì´í„° ì €ìž¥ ì¤‘...")
    if not os.path.exists(os.path.join('data', now_year, f"{now_year}_{now_month}.csv")):
        df_all_region.to_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"), header=True, index=False)
    else:
        data_final = pd.read_csv(os.path.join('data', now_year, f"{now_year}_{now_month}.csv"))
        data_final = pd.concat([data_final, df_all_region], ignore_index=True)
        data_final.drop_duplicates(inplace=True)
        data_final.to_csv(os.path.join('data', now_year, f"{now_year}_{now_month}_short_term.csv"), index=False)


def main():
    download_ultra_short_data()
    # download_short_term_data()


if __name__ == "__main__":
    main()
